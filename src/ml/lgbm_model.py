# =============================================================================
# LIGHTGBM WALK-FORWARD MODEL â€” EÄÄ°TÄ°M / TAHMÄ°N / RETRAIN
# =============================================================================
# AmaÃ§: GeÃ§miÅŸ trade sonuÃ§larÄ±ndan Ã¶ÄŸrenerek yeni sinyallerin karlÄ±lÄ±ÄŸÄ±nÄ±
#        tahmin etmek. Gemini'nin "semantik onay" rolÃ¼nÃ¼ veriye dayalÄ±
#        olasÄ±lÄ±ksal tahminle deÄŸiÅŸtirmek.
#
# Gemini vs LightGBM:
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ Ã–zellik            â”‚ Gemini               â”‚ LightGBM               â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ Ã–ÄŸrenme            â”‚ Yok (stateless)      â”‚ GeÃ§miÅŸ trade'lerden    â”‚
# â”‚ Karar tipi         â”‚ Semantik prompt       â”‚ OlasÄ±lÄ±ksal tahmin     â”‚
# â”‚ Feedback loop      â”‚ Yok                   â”‚ Her retrain'de iyileÅŸirâ”‚
# â”‚ Deterministik      â”‚ HayÄ±r (temperature)   â”‚ Evet (seed fixed)      â”‚
# â”‚ Maliyet            â”‚ API call (quota)      â”‚ SÄ±fÄ±r (lokal)          â”‚
# â”‚ Cold start         â”‚ Yok (hemen Ã§alÄ±ÅŸÄ±r)   â”‚ Var (min 30 trade)     â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# Model DetaylarÄ±:
# - Task: Binary Classification (karlÄ±=1, zararlÄ±=0)
# - Validation: Walk-forward expanding window (temporal order korunur)
# - Purged Gap: EÄŸitim sonu ile test baÅŸÄ± arasÄ±nda 2 trade boÅŸluk (leakage korumasÄ±)
# - Calibration: Platt Scaling ile olasÄ±lÄ±k kalibrasyonu
# - Cold Start: Min 30 trade altÄ±nda IC-only fallback (model tahmini yapÄ±lmaz)
# - Regularization: L1/L2 + max_depth limit + early stopping
#
# KullanÄ±m:
#   from ml.lgbm_model import LGBMSignalModel
#   model = LGBMSignalModel()
#   model.train(feature_df, target_series)
#   result = model.predict(feature_vector)
# =============================================================================

import json                                    # Model meta verisi JSON olarak saklanÄ±r
import logging                                 # YapÄ±landÄ±rÄ±lmÄ±ÅŸ log mesajlarÄ±
import pickle                                  # Model serialization (joblib yerine â€” daha az baÄŸÄ±mlÄ±lÄ±k)
import numpy as np                             # SayÄ±sal hesaplamalar ve array iÅŸlemleri
import pandas as pd                            # DataFrame â€” feature matrix ve sonuÃ§ tablolarÄ±
from pathlib import Path                       # Platform-baÄŸÄ±msÄ±z dosya yollarÄ±
from typing import Dict, List, Optional, Tuple, Any  # Tip belirteÃ§leri
from dataclasses import dataclass, field       # YapÄ±landÄ±rÄ±lmÄ±ÅŸ veri sÄ±nÄ±flarÄ±
from datetime import datetime, timezone        # Model versiyonlama zamanÄ±
from copy import deepcopy                      # Model kopyalama (snapshot)

# LightGBM â€” gradient boosting framework
# Neden LightGBM (XGBoost / CatBoost deÄŸil)?
# 1. Histogram-based split â†’ hÄ±zlÄ± eÄŸitim (kÃ¼Ã§Ã¼k veri setlerinde bile verimli)
# 2. Native NaN handling â†’ imputation gereksiz
# 3. Native categorical support â†’ one-hot encoding ihtiyacÄ± azalÄ±r
# 4. Leaf-wise growth â†’ daha derin aÄŸaÃ§lar, daha az overfitting (max_depth ile kontrol)
try:
    import lightgbm as lgb                     # LightGBM ana kÃ¼tÃ¼phanesi
    HAS_LIGHTGBM = True                        # LightGBM kurulu
except ImportError:
    lgb = None                                 # LightGBM kurulu deÄŸil
    HAS_LIGHTGBM = False                       # Fallback: sklearn GradientBoosting

# Scikit-learn â€” calibration, metrikler ve fallback model
from sklearn.calibration import CalibratedClassifierCV  # Platt Scaling
from sklearn.ensemble import GradientBoostingClassifier  # LightGBM yoksa fallback model
from sklearn.metrics import (                  # DeÄŸerlendirme metrikleri
    accuracy_score,                            # DoÄŸruluk (baseline referans)
    roc_auc_score,                             # AUC-ROC (ranking kalitesi)
    brier_score_loss,                          # OlasÄ±lÄ±k kalibrasyonu Ã¶lÃ§Ã¼sÃ¼
    log_loss,                                  # Logaritmik kayÄ±p (olasÄ±lÄ±k kalitesi)
    precision_score,                           # Precision (yanlÄ±ÅŸ pozitif kontrolÃ¼)
    recall_score,                              # Recall (kaÃ§Ä±rÄ±lan fÄ±rsat kontrolÃ¼)
    f1_score,                                  # F1 (precision-recall dengesi)
    classification_report,                     # DetaylÄ± sÄ±nÄ±flandÄ±rma raporu
)

# Proje iÃ§i import
from .feature_engineer import (                # AdÄ±m 1'de oluÅŸturduÄŸumuz modÃ¼l
    MLDecision,                                # LONG/SHORT/WAIT enum
    MLDecisionResult,                          # Nihai karar objesi
    MLFeatureVector,                           # Feature vektÃ¶rÃ¼
)

# Logger
logger = logging.getLogger(__name__)


# =============================================================================
# SABÄ°TLER
# =============================================================================

MIN_SAMPLES_TRAIN = 30                         # Minimum eÄŸitim sample sayÄ±sÄ± (cold start eÅŸiÄŸi)
MIN_SAMPLES_POSITIVE = 10                      # Minimum pozitif sÄ±nÄ±f sample (class imbalance korumasÄ±)
PURGE_GAP = 2                                  # Walk-forward'da eÄŸitim-test arasÄ± boÅŸluk (temporal leakage korumasÄ±)
DEFAULT_SEED = 42                              # Tekrarlanabilirlik iÃ§in sabit random seed
MODEL_DIR = Path("models")                     # Model dosyalarÄ±nÄ±n kaydedileceÄŸi dizin


# =============================================================================
# MODEL PERFORMANS DATACLASS
# =============================================================================

@dataclass
class ModelMetrics:
    """
    Model eÄŸitim/validasyon metrikleri.
    
    Her retrain sonrasÄ± bu obje oluÅŸturulur ve loglanÄ±r.
    Zaman iÃ§inde metriklerin seyri takip edilir (model degradation kontrolÃ¼).
    """
    # SÄ±nÄ±flandÄ±rma metrikleri
    accuracy: float = 0.0                      # DoÄŸruluk oranÄ± (baseline)
    auc_roc: float = 0.5                       # AUC-ROC (0.5 = rastgele, 1.0 = mÃ¼kemmel)
    brier_score: float = 0.5                   # Brier Score (dÃ¼ÅŸÃ¼k = iyi kalibrasyon)
    log_loss_val: float = 1.0                  # Log Loss (dÃ¼ÅŸÃ¼k = iyi olasÄ±lÄ±k tahmini)
    precision: float = 0.0                     # Precision (trade aÃ§tÄ±ÄŸÄ±nda doÄŸruluk)
    recall: float = 0.0                        # Recall (karlÄ± fÄ±rsatlarÄ± yakalama)
    f1: float = 0.0                            # F1 Score (precision-recall dengesi)

    # Veri bilgisi
    n_train: int = 0                           # EÄŸitim sample sayÄ±sÄ±
    n_val: int = 0                             # Validasyon sample sayÄ±sÄ±
    n_positive: int = 0                        # Pozitif sÄ±nÄ±f (karlÄ± trade) sayÄ±sÄ±
    positive_rate: float = 0.0                 # Pozitif sÄ±nÄ±f oranÄ± (base rate)

    # Model bilgisi
    n_features: int = 0                        # KullanÄ±lan feature sayÄ±sÄ±
    best_iteration: int = 0                    # Early stopping'de en iyi iterasyon
    feature_importance_top5: List[str] = field(default_factory=list)  # En Ã¶nemli 5 feature

    # Meta
    model_version: str = ""                    # EÄŸitim tarihi bazlÄ± versiyon
    trained_at: str = ""                       # EÄŸitim zamanÄ± (UTC ISO)

    def is_usable(self) -> bool:
        """
        Model kullanÄ±labilir durumda mÄ±?
        
        Kriterler:
        1. AUC > 0.52 â€” rastgeleden Ã¶lÃ§Ã¼lebilir ÅŸekilde iyi (0.5 = rastgele)
        2. Yeterli sample ile eÄŸitilmiÅŸ (n_train >= MIN_SAMPLES_TRAIN)
        3. Brier score makul (< 0.35 â€” kÃ¶tÃ¼ kalibrasyon deÄŸil)
        
        Not: 0.52 kasÄ±tlÄ± olarak dÃ¼ÅŸÃ¼k tutuluyor Ã§Ã¼nkÃ¼:
        - Kripto'da edge Ã§ok kÃ¼Ã§Ã¼k olabilir
        - BaÅŸlangÄ±Ã§ta az veriyle eÄŸitilecek
        - Zamanla iyileÅŸmesi bekleniyor
        """
        return (
            self.auc_roc > 0.52                # Rastgeleden anlamlÄ± fark
            and self.n_train >= MIN_SAMPLES_TRAIN  # Yeterli veri
            and self.brier_score < 0.35        # Makul kalibrasyon
        )

    def summary(self) -> str:
        """Telegram / log iÃ§in okunabilir Ã¶zet."""
        status = "âœ… KullanÄ±labilir" if self.is_usable() else "âš ï¸ Yetersiz"
        return (
            f"ğŸ“Š Model Metrikleri ({status})\n"
            f"  AUC: {self.auc_roc:.3f} | F1: {self.f1:.3f} | Brier: {self.brier_score:.3f}\n"
            f"  Precision: {self.precision:.3f} | Recall: {self.recall:.3f}\n"
            f"  Train: {self.n_train} | Val: {self.n_val} | +Rate: {self.positive_rate:.1%}\n"
            f"  Top Features: {', '.join(self.feature_importance_top5[:3])}"
        )


# =============================================================================
# ANA MODEL SINIFI
# =============================================================================

class LGBMSignalModel:
    """
    LightGBM binary classification modeli â€” trade karlÄ±lÄ±k tahmini.
    
    Pipeline:
    1. train()   : GeÃ§miÅŸ trade feature'larÄ± + sonuÃ§larÄ± ile model eÄŸit
    2. predict() : Yeni sinyal iÃ§in karlÄ±lÄ±k olasÄ±lÄ±ÄŸÄ± tahmin et
    3. retrain() : Yeni trade verileri gelince modeli gÃ¼ncelle
    4. save/load : Model persist (disk'e kaydet / diskten yÃ¼kle)
    
    Walk-Forward Validasyon:
    - EÄŸitim verisi zamana gÃ¶re sÄ±ralÄ± (temporal order)
    - Son %20 validasyon, ilk %80 eÄŸitim (expanding window)
    - Purge gap: eÄŸitim sonu ile validasyon baÅŸÄ± arasÄ±nda 2 sample boÅŸluk
    - Bu sayede temporal leakage riski minimize edilir
    
    Cold Start Stratejisi:
    - < 30 trade: Model eÄŸitilmez, IC-only fallback kullanÄ±lÄ±r
    - 30-100 trade: Basit model (az aÄŸaÃ§, yÃ¼ksek regularization)
    - > 100 trade: Tam model (daha fazla aÄŸaÃ§, daha az regularization)
    """

    def __init__(
        self,
        model_dir: Optional[Path] = None,      # Model kayÄ±t dizini
        seed: int = DEFAULT_SEED,              # Random seed (tekrarlanabilirlik)
        verbose: bool = True,                  # DetaylÄ± log mesajlarÄ±
    ):
        """
        LGBMSignalModel baÅŸlatÄ±r.
        
        Parameters:
        ----------
        model_dir : Path, optional
            Model dosyalarÄ±nÄ±n kaydedileceÄŸi dizin
            None â†’ proje kÃ¶kÃ¼nde 'models/' kullanÄ±lÄ±r
            
        seed : int
            Random seed â€” aynÄ± veri ile aynÄ± model Ã§Ä±ktÄ±sÄ± garanti
            
        verbose : bool
            True â†’ eÄŸitim/tahmin detaylarÄ±nÄ± logla
        """
        self.model_dir = model_dir or MODEL_DIR  # Model kayÄ±t dizini
        self.seed = seed                       # Tekrarlanabilirlik seed'i
        self.verbose = verbose                 # Log detay seviyesi

        # Model state
        self.model: Optional[lgb.LGBMClassifier] = None        # EÄŸitilmiÅŸ LightGBM modeli
        self.calibrator: Optional[CalibratedClassifierCV] = None  # Platt Scaling kalibrasyonu
        self.feature_names: List[str] = []     # EÄŸitimde kullanÄ±lan feature isimleri (sÄ±ralÄ±)
        self.metrics: Optional[ModelMetrics] = None  # Son eÄŸitimin metrikleri
        self.is_trained: bool = False          # Model eÄŸitilmiÅŸ mi?
        self.model_version: str = ""           # Versiyon string'i (tarih bazlÄ±)

        # EÄŸitim geÃ§miÅŸi (model degradation takibi)
        self.metrics_history: List[ModelMetrics] = []  # TÃ¼m eÄŸitimlerin metrikleri

        # Model dizinini oluÅŸtur
        self.model_dir.mkdir(parents=True, exist_ok=True)

    # =========================================================================
    # LIGHTGBM HÄ°PERPARAMETRELER
    # =========================================================================

    def _get_params(self, n_samples: int) -> Dict[str, Any]:
        """
        Veri bÃ¼yÃ¼klÃ¼ÄŸÃ¼ne gÃ¶re adaptif hiperparametreler.
        
        KÃ¼Ã§Ã¼k veri (30-100 sample):
        - Az aÄŸaÃ§ (50) + yÃ¼ksek regularization â†’ overfitting korumasÄ±
        - SÄ±ÄŸ aÄŸaÃ§lar (max_depth=3) â†’ basit karar kurallarÄ±
        - YÃ¼ksek min_child_samples â†’ her yaprak yeterli Ã¶rnekle destekli
        
        BÃ¼yÃ¼k veri (100+ sample):
        - Daha fazla aÄŸaÃ§ (200) + orta regularization
        - Daha derin aÄŸaÃ§lar (max_depth=5) â†’ karmaÅŸÄ±k etkileÅŸimler
        - DÃ¼ÅŸÃ¼k min_child_samples â†’ daha ince granÃ¼larite
        
        LightGBM yoksa sklearn GradientBoosting parametreleri dÃ¶ndÃ¼rÃ¼lÃ¼r.
        
        Parameters:
        ----------
        n_samples : int
            Toplam eÄŸitim sample sayÄ±sÄ±
            
        Returns:
        -------
        Dict[str, Any]
            Model parametre dict'i (LightGBM veya sklearn formatÄ±nda)
        """
        if HAS_LIGHTGBM:
            # â”€â”€ LightGBM Parametreleri â”€â”€
            if n_samples < 100:
                return {
                    'objective': 'binary',             # Binary classification (karlÄ±/zararlÄ±)
                    'metric': 'binary_logloss',        # Optimizasyon metriÄŸi: log loss
                    'boosting_type': 'gbdt',           # Gradient Boosted Decision Trees
                    'n_estimators': 50,                # Az aÄŸaÃ§ â€” overfitting riski dÃ¼ÅŸÃ¼k
                    'max_depth': 3,                    # SÄ±ÄŸ aÄŸaÃ§ â€” basit karar kurallarÄ±
                    'num_leaves': 7,                   # 2^3 - 1 = 7 (max_depth ile tutarlÄ±)
                    'learning_rate': 0.05,             # YavaÅŸ Ã¶ÄŸrenme â€” daha stabil
                    'min_child_samples': 10,           # Her yaprakta min 10 sample
                    'reg_alpha': 1.0,                  # L1 regularization (feature selection etkisi)
                    'reg_lambda': 2.0,                 # L2 regularization (aÄŸÄ±rlÄ±k shrinkage)
                    'subsample': 0.8,                  # Her aÄŸaÃ§ta %80 sample kullan (bagging)
                    'colsample_bytree': 0.8,           # Her aÄŸaÃ§ta %80 feature kullan
                    'min_split_gain': 0.01,            # Minimum split kazancÄ±
                    'random_state': self.seed,         # Tekrarlanabilirlik
                    'verbose': -1,                     # LightGBM sessiz mod
                    'is_unbalance': True,              # Class imbalance otomatik aÄŸÄ±rlÄ±klama
                }
            return {
                'objective': 'binary',                 # Binary classification
                'metric': 'binary_logloss',            # Log loss
                'boosting_type': 'gbdt',               # GBDT
                'n_estimators': 200,                   # Daha fazla aÄŸaÃ§
                'max_depth': 5,                        # Orta derinlik
                'num_leaves': 20,                      # Daha fazla yaprak
                'learning_rate': 0.03,                 # Daha yavaÅŸ Ã¶ÄŸrenme
                'min_child_samples': 5,                # Daha ince granÃ¼larite
                'reg_alpha': 0.5,                      # Orta L1
                'reg_lambda': 1.0,                     # Orta L2
                'subsample': 0.85,                     # %85 sample bagging
                'colsample_bytree': 0.85,              # %85 feature bagging
                'min_split_gain': 0.005,               # Daha dÃ¼ÅŸÃ¼k split eÅŸiÄŸi
                'random_state': self.seed,             # Tekrarlanabilirlik
                'verbose': -1,                         # Sessiz mod
                'is_unbalance': True,                  # Class imbalance handling
            }
        else:
            # â”€â”€ sklearn GradientBoosting Parametreleri (LightGBM yoksa fallback) â”€â”€
            if n_samples < 100:
                return {
                    'n_estimators': 50,                # Az aÄŸaÃ§
                    'max_depth': 3,                    # SÄ±ÄŸ aÄŸaÃ§
                    'learning_rate': 0.05,             # YavaÅŸ Ã¶ÄŸrenme
                    'min_samples_leaf': 10,            # Min yaprak sample (lgb: min_child_samples)
                    'subsample': 0.8,                  # Stochastic gradient boosting
                    'max_features': 0.8,               # Feature bagging (lgb: colsample_bytree)
                    'random_state': self.seed,         # Tekrarlanabilirlik
                }
            return {
                'n_estimators': 200,                   # Daha fazla aÄŸaÃ§
                'max_depth': 5,                        # Orta derinlik
                'learning_rate': 0.03,                 # Daha yavaÅŸ Ã¶ÄŸrenme
                'min_samples_leaf': 5,                 # Daha ince granÃ¼larite
                'subsample': 0.85,                     # %85 stochastic
                'max_features': 0.85,                  # %85 feature
                'random_state': self.seed,             # Tekrarlanabilirlik
            }

    # =========================================================================
    # EÄÄ°TÄ°M (TRAIN)
    # =========================================================================

    def train(
        self,
        X: pd.DataFrame,                      # Feature matrix (satÄ±r=trade, kolon=feature)
        y: pd.Series,                          # Hedef (1=karlÄ±, 0=zararlÄ±)
        val_ratio: float = 0.2,               # Validasyon oranÄ± (son %20)
        purge_gap: int = PURGE_GAP,           # EÄŸitim-validasyon arasÄ± boÅŸluk
    ) -> ModelMetrics:
        """
        Walk-forward expanding window ile model eÄŸit.
        
        Walk-Forward Neden?
        - Zaman serisi verisinde random split â†’ temporal leakage
        - Walk-forward: eÄŸitim DAÄ°MA geÃ§miÅŸ, validasyon DAÄ°MA gelecek
        - Bu sayede gerÃ§ek dÃ¼nya performansÄ±na daha yakÄ±n tahmin
        
        Purge Gap Neden?
        - ArdÄ±ÅŸÄ±k trade'ler aynÄ± piyasa koÅŸullarÄ±nda aÃ§Ä±lmÄ±ÅŸ olabilir
        - EÄŸitim sonundaki trade ile validasyon baÅŸÄ±ndaki trade korelasyonlu olabilir
        - 2 trade boÅŸluk bÄ±rakarak bu temporal sÄ±zÄ±ntÄ±yÄ± azaltÄ±rÄ±z
        
        Parameters:
        ----------
        X : pd.DataFrame
            Feature matrix â€” feature_engineer.build_batch_features() Ã§Ä±ktÄ±sÄ±
            Meta kolonlar (_symbol, _coin, _timestamp) dahil olabilir, filtrelenir
            
        y : pd.Series
            Hedef deÄŸiÅŸken â€” 1=karlÄ± trade (net_pnl > 0), 0=zararlÄ± trade
            
        val_ratio : float
            Validasyon seti oranÄ± (varsayÄ±lan 0.2 = son %20)
            
        purge_gap : int
            EÄŸitim-validasyon arasÄ± boÅŸluk (temporal leakage korumasÄ±)
            
        Returns:
        -------
        ModelMetrics
            EÄŸitim ve validasyon metrikleri
        """
        # â”€â”€ 0. Veri Validasyonu â”€â”€
        # Meta kolonlarÄ± (_ile baÅŸlayanlar) feature'lardan ayÄ±r
        feature_cols = [c for c in X.columns if not c.startswith('_')]
        X_clean = X[feature_cols].copy()       # Sadece feature kolonlarÄ±
        self.feature_names = feature_cols       # Feature isimlerini kaydet

        n_total = len(X_clean)                 # Toplam sample sayÄ±sÄ±
        n_positive = int(y.sum())              # KarlÄ± trade sayÄ±sÄ±

        # Minimum sample kontrolÃ¼ (cold start)
        if n_total < MIN_SAMPLES_TRAIN:
            logger.warning(
                f"âš ï¸ Yetersiz veri: {n_total} < {MIN_SAMPLES_TRAIN} minimum. "
                f"Model eÄŸitilmeyecek, IC-only fallback kullanÄ±lacak."
            )
            return self._empty_metrics(n_total, n_positive)

        # Minimum pozitif sÄ±nÄ±f kontrolÃ¼ (class imbalance)
        if n_positive < MIN_SAMPLES_POSITIVE:
            logger.warning(
                f"âš ï¸ Yetersiz pozitif sÄ±nÄ±f: {n_positive} < {MIN_SAMPLES_POSITIVE}. "
                f"Model dengesiz olabilir."
            )
            # EÄŸitime devam et ama uyar (is_unbalance=True bunu handle eder)

        # â”€â”€ 1. Walk-Forward Split â”€â”€
        # Temporal order korunuyor â€” SON val_ratio kadarÄ± validasyon
        val_size = max(int(n_total * val_ratio), 5)  # Min 5 validasyon sample
        train_end = n_total - val_size - purge_gap   # Purge gap bÄ±rak

        if train_end < MIN_SAMPLES_TRAIN:
            # Purge gap Ã§Ä±karÄ±nca eÄŸitim seti Ã§ok kÃ¼Ã§Ã¼k kaldÄ±
            train_end = n_total - val_size     # Purge gap'i kaldÄ±r (az veri durumu)
            purge_gap = 0

        X_train = X_clean.iloc[:train_end]             # EÄŸitim feature'larÄ±
        y_train = y.iloc[:train_end]                    # EÄŸitim hedefi
        X_val = X_clean.iloc[train_end + purge_gap:]   # Validasyon feature'larÄ± (purge gap sonrasÄ±)
        y_val = y.iloc[train_end + purge_gap:]          # Validasyon hedefi

        if self.verbose:
            logger.info(
                f"  ğŸ“ Walk-Forward Split: "
                f"Train={len(X_train)} | Purge={purge_gap} | Val={len(X_val)} | "
                f"+Rate: {y_train.mean():.1%} (train) / {y_val.mean():.1%} (val)"
            )

        # â”€â”€ 2. Model EÄŸitimi â”€â”€
        params = self._get_params(len(X_train))  # Adaptif hiperparametreler

        if HAS_LIGHTGBM:
            # LightGBM â€” native NaN handling, early stopping callback
            self.model = lgb.LGBMClassifier(**params)

            # Early stopping callback: validasyon loss iyileÅŸmezse dur
            callbacks = [
                lgb.early_stopping(stopping_rounds=10, verbose=False),
                lgb.log_evaluation(period=-1),
            ]

            self.model.fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)],
                callbacks=callbacks,
            )
        else:
            # sklearn GradientBoosting â€” NaN'larÄ± median ile doldur (native NaN desteÄŸi yok)
            self._impute_median = X_train.median()         # Median deÄŸerleri sakla (predict'te de lazÄ±m)
            X_train_filled = X_train.fillna(self._impute_median)
            X_val_filled = X_val.fillna(self._impute_median)

            self.model = GradientBoostingClassifier(**params)
            self.model.fit(X_train_filled, y_train)

            # sklearn'de early stopping yok â†’ n_estimators sabit kalÄ±r
            # Validasyon seti kalibrasyon iÃ§in kullanÄ±lacak

        # â”€â”€ 3. OlasÄ±lÄ±k Kalibrasyonu (Platt Scaling) â”€â”€
        # LightGBM'in ham olasÄ±lÄ±klarÄ± genellikle well-calibrated deÄŸil
        # Platt Scaling: sigmoid fit ile kalibre et
        try:
            X_val_for_cal = X_val if HAS_LIGHTGBM else X_val.fillna(self._impute_median)

            # sklearn >= 1.6'da 'prefit' kaldÄ±rÄ±ldÄ±, cv=2 ile Ã§alÄ±ÅŸtÄ±r
            # prefit destekliyorsa onu kullan (daha doÄŸru), yoksa mini cv
            try:
                self.calibrator = CalibratedClassifierCV(
                    self.model, method='sigmoid', cv='prefit',
                )
                self.calibrator.fit(X_val_for_cal, y_val)
            except (ValueError, TypeError):
                # prefit desteklenmiyorsa â†’ 2-fold CV ile kalibre et
                # Not: Bu eÄŸitim setinde yeniden fit yapar ama Platt Scaling
                # sadece sigmoid parametreleri Ã¶ÄŸreniyor, model aÄŸÄ±rlÄ±klarÄ± deÄŸiÅŸmiyor
                X_cal = pd.concat([X_train if HAS_LIGHTGBM else X_train.fillna(self._impute_median),
                                   X_val_for_cal], ignore_index=True)
                y_cal = pd.concat([y_train, y_val], ignore_index=True)
                self.calibrator = CalibratedClassifierCV(
                    estimator=GradientBoostingClassifier(**params) if not HAS_LIGHTGBM else lgb.LGBMClassifier(**params),
                    method='sigmoid', cv=2,
                )
                self.calibrator.fit(X_cal, y_cal)
        except Exception as e:
            logger.warning(f"âš ï¸ Kalibrasyon baÅŸarÄ±sÄ±z: {e}. Ham olasÄ±lÄ±klar kullanÄ±lacak.")
            self.calibrator = None

        # â”€â”€ 4. Metrik Hesaplama â”€â”€
        metrics = self._evaluate(X_val, y_val, X_train, y_train)
        self.metrics = metrics                 # Son metrikleri kaydet
        self.metrics_history.append(metrics)   # GeÃ§miÅŸe ekle
        self.is_trained = True                 # Model kullanÄ±labilir

        # Versiyon string'i oluÅŸtur (tarih bazlÄ±)
        self.model_version = datetime.now(timezone.utc).strftime("v%Y%m%d_%H%M%S")
        metrics.model_version = self.model_version
        metrics.trained_at = datetime.now(timezone.utc).isoformat()

        if self.verbose:
            logger.info(f"\n{metrics.summary()}")

        return metrics

    # =========================================================================
    # TAHMÄ°N (PREDICT)
    # =========================================================================

    def predict(
        self,
        feature_vector: 'MLFeatureVector',     # AdÄ±m 1'deki feature vektÃ¶rÃ¼
        ic_score: float = 0.0,                # IC composite skoru (gate keeper iÃ§in)
        ic_direction: str = "NEUTRAL",         # IC yÃ¶nÃ¼ (fallback iÃ§in)
        gate_thresholds: Optional[Dict] = None,  # Gate keeper eÅŸikleri
    ) -> MLDecisionResult:
        """
        Yeni sinyal iÃ§in karlÄ±lÄ±k tahmini yap ve karar dÃ¶ndÃ¼r.
        
        Pipeline:
        1. Gate Keeper kontrolÃ¼ (IC eÅŸikleri â€” mevcut sistemle aynÄ±)
        2. Feature vektÃ¶rÃ¼nÃ¼ numpy array'e Ã§evir
        3. LightGBM predict_proba() â†’ karlÄ±lÄ±k olasÄ±lÄ±ÄŸÄ±
        4. Kalibrasyon uygula (varsa)
        5. OlasÄ±lÄ±k â†’ karar mapping (eÅŸik bazlÄ±)
        6. MLDecisionResult oluÅŸtur
        
        Cold Start: Model eÄŸitilmemiÅŸse IC-only fallback kullanÄ±lÄ±r.
        
        Parameters:
        ----------
        feature_vector : MLFeatureVector
            AdÄ±m 1'de oluÅŸturulan feature vektÃ¶rÃ¼
            
        ic_score : float
            IC composite skoru (0-100) â€” gate keeper kontrolÃ¼ iÃ§in
            
        ic_direction : str
            IC'nin Ã¶nerdiÄŸi yÃ¶n ('LONG'/'SHORT'/'NEUTRAL')
            
        gate_thresholds : Dict, optional
            Gate keeper eÅŸikleri {'no_trade': 40, 'full_trade': 70}
            None â†’ varsayÄ±lan deÄŸerler kullanÄ±lÄ±r
            
        Returns:
        -------
        MLDecisionResult
            Karar + gÃ¼ven + gerekÃ§e (execution modÃ¼lÃ¼ne gÃ¶nderilir)
        """
        # VarsayÄ±lan gate eÅŸikleri
        if gate_thresholds is None:
            gate_thresholds = {'no_trade': 40, 'full_trade': 70}

        # â”€â”€ 1. Gate Keeper â”€â”€
        # IC eÅŸik kontrolÃ¼ (mevcut sistemle aynÄ± mantÄ±k)
        gate_action = self._check_gate(ic_score, gate_thresholds)

        if gate_action == "NO_TRADE":
            return MLDecisionResult(
                decision=MLDecision.WAIT,
                confidence=max(ic_score * 0.5, 10),  # DÃ¼ÅŸÃ¼k gÃ¼ven
                reasoning=f"IC skoru ({ic_score:.0f}) gate eÅŸiÄŸinin altÄ±nda ({gate_thresholds['no_trade']})",
                gate_action="NO_TRADE",
                ic_score=ic_score,
                model_version=self.model_version or "no_model",
            )

        # â”€â”€ 2. Cold Start KontrolÃ¼ â”€â”€
        # Model eÄŸitilmemiÅŸse veya yetersizse IC-only fallback
        if not self.is_trained or self.model is None:
            return self._ic_fallback(ic_score, ic_direction, gate_action)

        # Model metrikleri yetersizse uyar ama yine de tahmin yap
        if self.metrics and not self.metrics.is_usable():
            logger.warning(
                f"âš ï¸ Model metrikleri yetersiz (AUC={self.metrics.auc_roc:.3f}). "
                f"Tahmin yapÄ±lacak ama gÃ¼ven dÃ¼ÅŸÃ¼rÃ¼lecek."
            )

        # â”€â”€ 3. Feature Array HazÄ±rla â”€â”€
        try:
            feature_dict = feature_vector.to_dict()  # Feature dict

            # EÄŸitimdeki feature sÄ±ralamasÄ± ile aynÄ± mÄ± kontrol et
            X_pred = pd.DataFrame([feature_dict])[self.feature_names]
        except KeyError as e:
            logger.error(f"âŒ Feature uyumsuzluÄŸu: {e}")
            return self._ic_fallback(ic_score, ic_direction, gate_action)

        # â”€â”€ 4. Tahmin â”€â”€
        try:
            # sklearn fallback'te NaN'larÄ± median ile doldur
            if not HAS_LIGHTGBM and hasattr(self, '_impute_median'):
                X_pred = X_pred.fillna(self._impute_median)

            if self.calibrator is not None:
                # Kalibre edilmiÅŸ olasÄ±lÄ±k (daha gÃ¼venilir)
                prob = self.calibrator.predict_proba(X_pred)[0][1]
            else:
                # Ham model olasÄ±lÄ±ÄŸÄ±
                prob = self.model.predict_proba(X_pred)[0][1]
        except Exception as e:
            logger.error(f"âŒ Tahmin hatasÄ±: {e}")
            return self._ic_fallback(ic_score, ic_direction, gate_action)

        # â”€â”€ 5. OlasÄ±lÄ±k â†’ Karar Mapping â”€â”€
        confidence = prob * 100                # 0-1 â†’ 0-100 skala

        # Model metrikler yetersizse gÃ¼veni dÃ¼ÅŸÃ¼r
        if self.metrics and not self.metrics.is_usable():
            confidence *= 0.75                 # %25 penaltÄ±

        # Karar eÅŸikleri:
        # prob >= 0.55 â†’ IC yÃ¶nÃ¼nde iÅŸlem aÃ§ (kÃ¼Ã§Ã¼k edge bile deÄŸerli)
        # prob < 0.45  â†’ IC yÃ¶nÃ¼nÃ¼n tersi veya WAIT
        # 0.45-0.55    â†’ Belirsiz bÃ¶lge â†’ WAIT
        if prob >= 0.55:
            decision = MLDecision.from_direction(ic_direction)  # IC yÃ¶nÃ¼nde
        elif prob < 0.45:
            decision = MLDecision.WAIT                          # Model onaylamÄ±yor
        else:
            decision = MLDecision.WAIT                          # Belirsiz bÃ¶lge

        # â”€â”€ 6. Feature Importance (top 3) â”€â”€
        top3_features = self._get_top_features(3)

        # â”€â”€ 7. GerekÃ§e OluÅŸtur â”€â”€
        reasoning = self._build_reasoning(prob, ic_score, ic_direction, decision, top3_features)

        return MLDecisionResult(
            decision=decision,
            confidence=round(confidence, 1),
            reasoning=reasoning,
            gate_action=gate_action,
            ic_score=ic_score,
            model_version=self.model_version,
            feature_importance_top3=top3_features,
            timestamp=datetime.now(timezone.utc).isoformat(),
        )

    # =========================================================================
    # RETRAIN (YENÄ° VERÄ° Ä°LE GÃœNCELLEME)
    # =========================================================================

    def retrain(
        self,
        X: pd.DataFrame,                      # TÃ¼m mevcut feature matrix (eski + yeni)
        y: pd.Series,                          # TÃ¼m hedef deÄŸiÅŸken (eski + yeni)
    ) -> ModelMetrics:
        """
        Yeni trade verileri ile modeli sÄ±fÄ±rdan eÄŸit.
        
        Neden Incremental DeÄŸil?
        - LightGBM incremental learning destekler ama kÃ¼Ã§Ã¼k veri setlerinde
          sÄ±fÄ±rdan eÄŸitim daha stabil sonuÃ§ verir
        - 30-500 trade arasÄ±nda sÄ±fÄ±rdan eÄŸitim < 1 saniye sÃ¼rer
        - Incremental'da eski pattern'ler unutulabilir (catastrophic forgetting)
        
        Ã‡aÄŸrÄ±lma ZamanÄ±:
        - Her N trade kapandÄ±ÄŸÄ±nda (varsayÄ±lan: 5)
        - GÃ¼nlÃ¼k periyodik (cron/scheduler)
        - Manuel tetikleme
        
        Parameters:
        ----------
        X : pd.DataFrame
            TÃœM geÃ§miÅŸ trade'lerin feature matrix'i (trade_memory'den gelir)
            
        y : pd.Series
            TÃœM geÃ§miÅŸ trade'lerin hedefi (1=karlÄ±, 0=zararlÄ±)
            
        Returns:
        -------
        ModelMetrics
            Yeni eÄŸitimin metrikleri
        """
        if self.verbose:
            logger.info(
                f"ğŸ”„ Retrain baÅŸlÄ±yor: {len(X)} sample | "
                f"+Rate: {y.mean():.1%}"
            )

        # Eski modeli snapshot al (rollback iÃ§in)
        old_model = deepcopy(self.model)       # Derin kopya
        old_metrics = deepcopy(self.metrics)

        # SÄ±fÄ±rdan eÄŸit
        new_metrics = self.train(X, y)

        # Model degradation kontrolÃ¼
        # Yeni model eskisinden belirgin ÅŸekilde kÃ¶tÃ¼yse rollback yap
        if old_metrics and old_metrics.is_usable() and new_metrics.is_usable():
            if new_metrics.auc_roc < old_metrics.auc_roc - 0.05:
                # AUC 0.05'ten fazla dÃ¼ÅŸtÃ¼ â†’ rollback
                logger.warning(
                    f"âš ï¸ Model degradation! AUC: {old_metrics.auc_roc:.3f} â†’ {new_metrics.auc_roc:.3f}. "
                    f"Eski model korunuyor."
                )
                self.model = old_model
                self.metrics = old_metrics
                return old_metrics

        # Auto-save
        try:
            self.save()
        except Exception as e:
            logger.warning(f"âš ï¸ Model kayÄ±t hatasÄ±: {e}")

        return new_metrics

    # =========================================================================
    # FEATURE IMPORTANCE
    # =========================================================================

    def get_feature_importance(self) -> pd.DataFrame:
        """
        Feature importance tablosu dÃ¶ndÃ¼r.
        
        3 farklÄ± importance tipi:
        - split: KaÃ§ kez split'te kullanÄ±lmÄ±ÅŸ (sÄ±klÄ±k)
        - gain: Toplam bilgi kazancÄ± (kalite)
        - combined: Normalize split + gain ortalamasÄ± (genel Ã¶nem)
        
        Returns:
        -------
        pd.DataFrame
            Kolonlar: feature, split, gain, combined
            SÄ±ralama: combined (azalan)
        """
        if not self.is_trained or self.model is None:
            return pd.DataFrame()              # EÄŸitilmemiÅŸ â†’ boÅŸ

        if HAS_LIGHTGBM and hasattr(self.model, 'booster_'):
            # LightGBM â€” split ve gain bazlÄ± iki ayrÄ± importance
            split_imp = self.model.booster_.feature_importance(importance_type='split')
            gain_imp = self.model.booster_.feature_importance(importance_type='gain')

            df = pd.DataFrame({
                'feature': self.feature_names,
                'split': split_imp,
                'gain': gain_imp,
            })

            for col in ['split', 'gain']:
                total = df[col].sum()
                df[f'{col}_norm'] = df[col] / total if total > 0 else 0.0

            df['combined'] = (df['split_norm'] + df['gain_norm']) / 2
        else:
            # sklearn GradientBoosting â€” feature_importances_ (impurity-based)
            imp = self.model.feature_importances_
            df = pd.DataFrame({
                'feature': self.feature_names,
                'split': imp,                  # sklearn'de tek importance var
                'gain': imp,                   # AynÄ± deÄŸeri kopyala (uyumluluk)
            })
            total = imp.sum()
            df['split_norm'] = imp / total if total > 0 else 0.0
            df['gain_norm'] = df['split_norm']
            df['combined'] = df['split_norm']

        # Combined'a gÃ¶re sÄ±rala (en Ã¶nemli Ã¼stte)
        df = df.sort_values('combined', ascending=False).reset_index(drop=True)

        return df

    # =========================================================================
    # MODEL KAYIT / YÃœKLEME
    # =========================================================================

    def save(self, filepath: Optional[Path] = None) -> Path:
        """
        EÄŸitilmiÅŸ modeli diske kaydet.
        
        Kaydedilenler:
        1. LightGBM model (pickle)
        2. Calibrator (pickle)
        3. Feature names (JSON)
        4. Metrics (JSON)
        5. Versiyon bilgisi (JSON)
        
        Parameters:
        ----------
        filepath : Path, optional
            KayÄ±t dosya yolu. None â†’ model_dir/lgbm_signal_model.pkl
            
        Returns:
        -------
        Path
            Kaydedilen dosya yolu
        """
        if not self.is_trained:
            raise ValueError("Model eÄŸitilmemiÅŸ â€” kayÄ±t yapÄ±lamaz")

        save_path = filepath or self.model_dir / "lgbm_signal_model.pkl"
        save_path.parent.mkdir(parents=True, exist_ok=True)

        # Tek dosyada tÃ¼m state'i kaydet
        state = {
            'model': self.model,               # LightGBM veya sklearn model objesi
            'calibrator': self.calibrator,      # Platt Scaling objesi
            'feature_names': self.feature_names,  # Feature isimleri (sÄ±ralÄ±)
            'metrics': self.metrics,            # Son metrikler
            'model_version': self.model_version,  # Versiyon string'i
            'has_lightgbm': HAS_LIGHTGBM,      # Hangi engine ile eÄŸitildi
            'impute_median': getattr(self, '_impute_median', None),  # sklearn NaN dolgu deÄŸerleri
            'saved_at': datetime.now(timezone.utc).isoformat(),
        }

        with open(save_path, 'wb') as f:
            pickle.dump(state, f)              # Pickle ile serialize et

        # Meta bilgiyi ayrÄ±ca JSON olarak da kaydet (okunabilirlik)
        meta_path = save_path.with_suffix('.json')
        meta = {
            'model_version': self.model_version,
            'feature_names': self.feature_names,
            'n_features': len(self.feature_names),
            'is_usable': self.metrics.is_usable() if self.metrics else False,
            'auc_roc': self.metrics.auc_roc if self.metrics else 0,
            'n_train': self.metrics.n_train if self.metrics else 0,
            'saved_at': state['saved_at'],
        }
        with open(meta_path, 'w') as f:
            json.dump(meta, f, indent=2)

        if self.verbose:
            logger.info(f"ğŸ’¾ Model kaydedildi: {save_path}")

        return save_path

    def load(self, filepath: Optional[Path] = None) -> bool:
        """
        KaydedilmiÅŸ modeli diskten yÃ¼kle.
        
        Parameters:
        ----------
        filepath : Path, optional
            YÃ¼klenecek dosya yolu. None â†’ model_dir/lgbm_signal_model.pkl
            
        Returns:
        -------
        bool
            True = baÅŸarÄ±yla yÃ¼klendi, False = dosya yok veya hata
        """
        load_path = filepath or self.model_dir / "lgbm_signal_model.pkl"

        if not load_path.exists():
            logger.info(f"ğŸ“‚ Model dosyasÄ± bulunamadÄ±: {load_path}")
            return False

        try:
            with open(load_path, 'rb') as f:
                state = pickle.load(f)         # Deserialize et

            self.model = state['model']
            self.calibrator = state.get('calibrator')
            self.feature_names = state['feature_names']
            self.metrics = state.get('metrics')
            self.model_version = state.get('model_version', 'loaded')
            self.is_trained = True
            # sklearn NaN dolgu deÄŸerlerini restore et
            if state.get('impute_median') is not None:
                self._impute_median = state['impute_median']

            if self.verbose:
                logger.info(
                    f"ğŸ“‚ Model yÃ¼klendi: {load_path} | "
                    f"Version: {self.model_version} | "
                    f"Features: {len(self.feature_names)}"
                )

            return True

        except Exception as e:
            logger.error(f"âŒ Model yÃ¼kleme hatasÄ±: {e}")
            return False

    # =========================================================================
    # YARDIMCI METODLAR (PRIVATE)
    # =========================================================================

    def _check_gate(self, ic_score: float, thresholds: Dict) -> str:
        """
        Gate Keeper: IC skoru eÅŸik kontrolÃ¼.
        Mevcut sistemdeki GateAction enum'unun string karÅŸÄ±lÄ±ÄŸÄ±.
        """
        if ic_score < thresholds.get('no_trade', 40):
            return "NO_TRADE"                  # IC Ã§ok dÃ¼ÅŸÃ¼k â€” iÅŸlem yapma
        elif ic_score < thresholds.get('full_trade', 70):
            return "REPORT_ONLY"               # IC orta â€” sadece raporla
        else:
            return "FULL_TRADE"                # IC yÃ¼ksek â€” trade aÃ§Ä±labilir

    def _ic_fallback(
        self,
        ic_score: float,
        ic_direction: str,
        gate_action: str,
    ) -> MLDecisionResult:
        """
        Model eÄŸitilmemiÅŸken IC-only fallback karar.
        Gemini'deki _ic_fallback ile aynÄ± mantÄ±k.
        
        IC >= 70 ve net yÃ¶n â†’ IC yÃ¶nÃ¼nde dÃ¼ÅŸÃ¼k gÃ¼venle karar
        Aksi halde â†’ WAIT
        """
        if ic_score >= 70 and ic_direction in ['LONG', 'SHORT']:
            decision = MLDecision.from_direction(ic_direction)
            confidence = min(ic_score * 0.65, 60)  # Max %60 gÃ¼ven (fallback sÄ±nÄ±rÄ±)
            reasoning = (
                f"âš ï¸ ML model henÃ¼z eÄŸitilmemiÅŸ (cold start). "
                f"IC fallback: {ic_direction} (IC={ic_score:.0f}, gÃ¼ven dÃ¼ÅŸÃ¼rÃ¼lmÃ¼ÅŸ)"
            )
        else:
            decision = MLDecision.WAIT
            confidence = max(ic_score * 0.3, 10)
            reasoning = (
                f"ML model eÄŸitilmemiÅŸ ve IC skoru yetersiz ({ic_score:.0f}). "
                f"Ä°ÅŸlem yapÄ±lmÄ±yor."
            )

        return MLDecisionResult(
            decision=decision,
            confidence=round(confidence, 1),
            reasoning=reasoning,
            gate_action=gate_action if gate_action != "NO_TRADE" else "REPORT_ONLY",
            ic_score=ic_score,
            model_version="ic_fallback",
            timestamp=datetime.now(timezone.utc).isoformat(),
        )

    def _evaluate(
        self,
        X_val: pd.DataFrame,
        y_val: pd.Series,
        X_train: pd.DataFrame,
        y_train: pd.Series,
    ) -> ModelMetrics:
        """
        Model performans metriklerini hesapla.
        
        Validasyon seti Ã¼zerinde deÄŸerlendirme yapar.
        EÄŸitim seti metrikleri sadece overfitting kontrolÃ¼ iÃ§in.
        """
        metrics = ModelMetrics()

        # Tahmin olasÄ±lÄ±klarÄ± (NaN handling for sklearn)
        X_val_pred = X_val if HAS_LIGHTGBM else X_val.fillna(X_train.median())
        if self.calibrator is not None:
            y_prob = self.calibrator.predict_proba(X_val_pred)[:, 1]
        else:
            y_prob = self.model.predict_proba(X_val_pred)[:, 1]

        y_pred = (y_prob >= 0.5).astype(int)   # 0.5 eÅŸik ile sÄ±nÄ±flandÄ±rma

        # Metrikler
        metrics.accuracy = float(accuracy_score(y_val, y_pred))
        metrics.precision = float(precision_score(y_val, y_pred, zero_division=0))
        metrics.recall = float(recall_score(y_val, y_pred, zero_division=0))
        metrics.f1 = float(f1_score(y_val, y_pred, zero_division=0))
        metrics.brier_score = float(brier_score_loss(y_val, y_prob))
        metrics.log_loss_val = float(log_loss(y_val, y_prob, labels=[0, 1]))

        # AUC-ROC (en az 2 sÄ±nÄ±f gerekli)
        if len(y_val.unique()) >= 2:
            metrics.auc_roc = float(roc_auc_score(y_val, y_prob))
        else:
            metrics.auc_roc = 0.5              # Tek sÄ±nÄ±f â†’ anlamsÄ±z

        # Veri bilgisi
        metrics.n_train = len(X_train)
        metrics.n_val = len(X_val)
        metrics.n_positive = int(y_train.sum() + y_val.sum())
        metrics.positive_rate = float(y_train.mean())
        metrics.n_features = len(self.feature_names)

        # Best iteration (early stopping â€” sadece LightGBM'de var)
        if HAS_LIGHTGBM and hasattr(self.model, 'best_iteration_'):
            metrics.best_iteration = self.model.best_iteration_ or self.model.n_estimators
        else:
            metrics.best_iteration = getattr(self.model, 'n_estimators', 0)

        # Feature importance (top 5)
        fi = self.get_feature_importance()
        if len(fi) > 0:
            metrics.feature_importance_top5 = fi['feature'].head(5).tolist()

        return metrics

    def _get_top_features(self, n: int = 3) -> List[str]:
        """En Ã¶nemli N feature'Ä±n isimlerini dÃ¶ndÃ¼r."""
        fi = self.get_feature_importance()
        if len(fi) > 0:
            return fi['feature'].head(n).tolist()
        return []

    def _build_reasoning(
        self,
        prob: float,
        ic_score: float,
        ic_direction: str,
        decision: MLDecision,
        top_features: List[str],
    ) -> str:
        """TÃ¼rkÃ§e karar gerekÃ§esi oluÅŸtur."""
        parts = []

        # Model tahmini
        if prob >= 0.55:
            parts.append(f"Model karlÄ±lÄ±k olasÄ±lÄ±ÄŸÄ±: %{prob*100:.0f} (pozitif sinyal)")
        elif prob < 0.45:
            parts.append(f"Model karlÄ±lÄ±k olasÄ±lÄ±ÄŸÄ±: %{prob*100:.0f} (negatif sinyal)")
        else:
            parts.append(f"Model karlÄ±lÄ±k olasÄ±lÄ±ÄŸÄ±: %{prob*100:.0f} (belirsiz bÃ¶lge)")

        # IC bilgisi
        parts.append(f"IC: {ic_score:.0f}/100 yÃ¶n={ic_direction}")

        # Top features
        if top_features:
            parts.append(f"Etken: {', '.join(top_features[:2])}")

        # Model gÃ¼venilirliÄŸi
        if self.metrics:
            parts.append(f"Model AUC: {self.metrics.auc_roc:.2f}")

        return " | ".join(parts)

    def _empty_metrics(self, n_total: int, n_positive: int) -> ModelMetrics:
        """EÄŸitim yapÄ±lmadÄ±ÄŸÄ±nda boÅŸ metrik objesi dÃ¶ndÃ¼r."""
        return ModelMetrics(
            n_train=n_total,
            n_positive=n_positive,
            positive_rate=n_positive / n_total if n_total > 0 else 0,
            model_version="not_trained",
        )


# =============================================================================
# BAÄIMSIZ Ã‡ALIÅTIRMA TESTÄ°
# =============================================================================

if __name__ == "__main__":
    """
    ModÃ¼lÃ¼ tek baÅŸÄ±na test et:
      cd src && python -m ml.lgbm_model
    
    Sentetik trade verisi ile eÄŸitim, tahmin, save/load test eder.
    """
    import tempfile
    import logging

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s | %(levelname)-8s | %(message)s',
        datefmt='%H:%M:%S'
    )

    print("=" * 60)
    print("  ğŸŒ³ LIGHTGBM MODEL â€” BAÄIMSIZ TEST")
    print(f"  {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)

    # â”€â”€ Sentetik Feature Matrix OluÅŸtur â”€â”€
    np.random.seed(42)
    n_samples = 80                             # 80 sentetik trade
    n_features = 44                            # AdÄ±m 1'deki feature sayÄ±sÄ±

    from .feature_engineer import FeatureEngineer
    eng = FeatureEngineer(verbose=False)
    feature_names = eng.get_feature_names()

    # Rastgele feature matrix
    X = pd.DataFrame(
        np.random.randn(n_samples, len(feature_names)),
        columns=feature_names,
    )

    # Sentetik hedef: feature'larla korelasyonlu (model Ã¶ÄŸrenebilsin)
    # ic_confidence ve ctf_direction_agreement yÃ¼ksekse karlÄ± olma olasÄ±lÄ±ÄŸÄ± artar
    signal = (
        0.3 * X['ic_confidence'] +
        0.2 * X['ctf_direction_agreement'] +
        0.1 * X['px_momentum_5'] +
        np.random.randn(n_samples) * 0.5      # Noise
    )
    y = (signal > signal.median()).astype(int)  # Medyan Ã¼stÃ¼ = karlÄ±

    print(f"\n  Veri: {n_samples} sample Ã— {len(feature_names)} feature")
    print(f"  +Rate: {y.mean():.1%}")

    # â”€â”€ Model EÄŸitimi â”€â”€
    with tempfile.TemporaryDirectory() as tmpdir:
        model = LGBMSignalModel(model_dir=Path(tmpdir), verbose=True)

        print("\n  ğŸ”§ EÄŸitim baÅŸlÄ±yor...")
        metrics = model.train(X, y)

        print(f"\n  is_usable: {metrics.is_usable()}")
        print(f"  AUC: {metrics.auc_roc:.3f}")

        # â”€â”€ Tahmin Testi â”€â”€
        print("\n  ğŸ¯ Tahmin testi...")
        dummy_vec = MLFeatureVector()
        dummy_vec.ic_features = {k: v for k, v in zip(feature_names[:12], X.iloc[0, :12])}
        dummy_vec.market_features = {k: v for k, v in zip(feature_names[12:18], X.iloc[0, 12:18])}
        dummy_vec.cross_tf_features = {k: v for k, v in zip(feature_names[18:24], X.iloc[0, 18:24])}
        dummy_vec.price_features = {k: v for k, v in zip(feature_names[24:34], X.iloc[0, 24:34])}
        dummy_vec.risk_features = {k: v for k, v in zip(feature_names[34:39], X.iloc[0, 34:39])}
        dummy_vec.temporal_features = {k: v for k, v in zip(feature_names[39:], X.iloc[0, 39:])}

        result = model.predict(dummy_vec, ic_score=75.0, ic_direction="SHORT")
        print(f"  Karar: {result.decision.value} | GÃ¼ven: {result.confidence:.1f}")
        print(f"  Gate: {result.gate_action}")
        print(f"  GerekÃ§e: {result.reasoning}")

        # â”€â”€ Cold Start Testi â”€â”€
        print("\n  â„ï¸ Cold start testi...")
        cold_model = LGBMSignalModel(model_dir=Path(tmpdir), verbose=False)
        cold_result = cold_model.predict(dummy_vec, ic_score=75.0, ic_direction="LONG")
        assert cold_result.model_version == "ic_fallback", "Cold start fallback hatasÄ±"
        print(f"  Karar: {cold_result.decision.value} (fallback)")

        # â”€â”€ Save / Load Testi â”€â”€
        print("\n  ğŸ’¾ Save/Load testi...")
        save_path = model.save()
        print(f"  Kaydedildi: {save_path}")

        model2 = LGBMSignalModel(model_dir=Path(tmpdir), verbose=False)
        loaded = model2.load()
        assert loaded, "Model yÃ¼klenemedi"
        assert model2.is_trained, "YÃ¼klenen model trained deÄŸil"
        assert len(model2.feature_names) == len(feature_names), "Feature sayÄ±sÄ± uyumsuz"
        print(f"  YÃ¼klendi: {model2.model_version} | Features: {len(model2.feature_names)}")

        # YÃ¼klenen model ile tahmin
        result2 = model2.predict(dummy_vec, ic_score=75.0, ic_direction="SHORT")
        print(f"  YÃ¼klenen model tahmini: {result2.decision.value} | GÃ¼ven: {result2.confidence:.1f}")

        # â”€â”€ Feature Importance â”€â”€
        print("\n  ğŸ“Š Feature Importance (Top 10):")
        fi = model.get_feature_importance()
        for _, row in fi.head(10).iterrows():
            print(f"    {row['feature']:<30} combined={row['combined']:.4f}")

        # â”€â”€ Retrain Testi â”€â”€
        print("\n  ğŸ”„ Retrain testi...")
        # Yeni veri ekle
        X_new = pd.DataFrame(
            np.random.randn(20, len(feature_names)),
            columns=feature_names,
        )
        signal_new = 0.3 * X_new['ic_confidence'] + np.random.randn(20) * 0.5
        y_new = (signal_new > signal_new.median()).astype(int)

        X_all = pd.concat([X, X_new], ignore_index=True)
        y_all = pd.concat([y, y_new], ignore_index=True)

        retrain_metrics = model.retrain(X_all, y_all)
        print(f"  Retrain AUC: {retrain_metrics.auc_roc:.3f}")

    print(f"\n{'=' * 60}")
    print(f"  âœ… LIGHTGBM MODEL TESTÄ° TAMAMLANDI")
    print(f"{'=' * 60}")
